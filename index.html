<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Nicholas Clarke</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Nicholas Clarke, PhD — Independent researcher in AI alignment, moral discovery, and moral philosophy. Cambridge, UK." />
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      max-width: 760px;
      margin: 60px auto;
      padding: 0 20px;
      line-height: 1.65;
      color: #111;
    }
    h1 { font-size: 2.1em; margin-bottom: 0.2em; letter-spacing: -0.02em; }
    h2 { font-size: 1.2em; margin-top: 2.2em; margin-bottom: 0.6em; }
    p { margin: 0.6em 0; }
    ul { padding-left: 1.2em; margin: 0.4em 0; }
    li { margin: 0.4em 0; }
    a { color: #0057b8; text-decoration: none; }
    a:hover { text-decoration: underline; }
    .tagline { color: #444; margin-bottom: 1em; }
    .section-note { color: #444; font-size: 0.98em; }
    .divider { height: 1px; background: #e6e6e6; margin: 2.5em 0; }
    footer { margin-top: 2.5em; color: #444; font-size: 0.95em; }
  </style>
</head>

<body>

<h1>Nicholas Clarke</h1>
<p class="tagline">
  <strong>Independent Researcher</strong> — AI Alignment &amp; Moral Philosophy<br />
  Cambridge, UK
</p>

<p>
I work on AI alignment, ethics, and long-term risk from a philosophical and behavioural-science
perspective. My central focus is a safety-first approach that treats
<strong>moral discovery and value idealisation as an empirical problem</strong>,
rather than assuming alignment targets in advance.
</p>

<p>
I am particularly interested in how AI systems might help identify and converge on what humans
would endorse under conditions of improved information, rational deliberation, and coherence —
and what this implies for alignment, corrigibility, and institutional design.
</p>

<h2>Research focus</h2>
<p class="section-note">
AI alignment and safety; moral discovery and value idealisation; idealised human preferences as
alignment targets; naturalistic moral theory (Goal Theory); epistemology and the theory of reasons
under conditions of epistemic non-ideality.
</p>

<h2>Background &amp; credentials</h2>
<ul>
  <li>PhD in Philosophy, University of London (awarded with no corrections)</li>
  <li>Independent researcher based in Cambridge</li>
  <li>Former software developer and technical author</li>
  <li>Current role: Reporting &amp; Information Analyst, University of Cambridge</li>
</ul>

<p class="section-note">
  I have an active academic research programme, with multiple completed and submitted papers across
  AI alignment, moral theory, and epistemology, alongside public writing on AI alignment and moral
  philosophy on <a href="https://uncommonwisdom.substack.com/" rel="noopener">Substack</a>.
</p>

<h2>Current projects</h2>
<ul>
  <li>
    <strong>AI-Assisted Discovery of Idealised Human Desire</strong><br />
    A research programme reframing AI alignment as moral discovery rather than behavioural control,
    with attention to robustness, manipulation, and deceptive alignment. I am currently applying
    for pilot funding to support a 6–9 month exploratory phase.
  </li>
  <li>
    <strong>Goal Theory (metaethics)</strong><br />
    A naturalistic moral framework grounding normativity in idealised human desire and causal
    structure, developed in my doctoral work and ongoing publications.
  </li>
  <li>
    <strong>Epistemology &amp; reasons under non-ideal conditions</strong><br />
    Independent work on non-factive operational accounts of reasons and knowledge suited to
    deliberation under uncertainty and fallible cognition.
  </li>
  <li>
    <strong>Public writing on AI &amp; ethics</strong><br />
    Essays aimed at bridging analytic philosophy, AI safety, and real-world decision-making.
  </li>
</ul>

<h2>Selected writing</h2>
<ul>
  <li>
    <em>Beyond Behavioural Control: Idealised Human Desire as an Alignment Target in AI</em>
    — submitted to <em>Minds and Machines</em>
  </li>
  <li>
    Essays on AI alignment, ethics, and moral philosophy on
    <a href="https://uncommonwisdom.substack.com/" rel="noopener">Substack</a>
  </li>
</ul>

<h2>Selected research (programmes & outputs)</h2>
<ul>
  <li>
    <strong>Goal Theory (moral philosophy)</strong> — a naturalistic framework for morality, with applications to moral
    disagreement, moral luck, non-identity, and metaethical semantics.
    <br />
    <span class="section-note">
      Multiple completed academic papers and submissions developed from this work.
    </span>
  </li>
  <li>
    <strong>Epistemology &amp; reasons</strong> — independent work developing non-factive operational accounts of reasons
    and knowledge suited to deliberation under epistemic uncertainty.
    <br />
    <span class="section-note">
      Several completed papers addressing factivity, operational knowledge, and practical reasoning.
    </span>
  </li>
</ul>
<p class="section-note">
  Drafts, preprints, and submission-ready manuscripts are available on request.
</p>


<div class="divider"></div>

<h2>Links</h2>
<ul>
  <li><a href="CV.pdf">CV (PDF)</a></li>
  <li><a href="https://uncommonwisdom.substack.com/" rel="noopener">Substack (essays)</a></li>
  <li><a href="mailto:nclarke431@gmail.com">Email</a></li>
</ul>

<footer>
  <p>
    If you’re reaching out, a short note with context (who you are, what you’re interested in, and what you’d like to discuss)
    is ideal.
  </p>
</footer>

</body>
</html>
